{
  "name": "VAE rec",
  "tagline": "Variational Recurrent Auto Encoder",
  "body": "#Variational Recurrent Auto Encoder\r\nIn this post, we discussed the variational auto encoder. This post extends this idea to data with sequential nature in a recurrent network. Auto encoders belong to the family of variational inference. In normal networks we define deterministic functions and backpropagate over the gradients. In variational inference, our network contains stochastic or information layers. Also we regard the cost function from a Bayesian standpoint. All together, this makes a likelihood of our data under the model. This likelihood turns out to be intractable. Therefore, we optimize a bound on this quantity. Hence, variational inference.\r\n\r\n#Computation graph\r\nAn auto encoder always follows a similar structure: an encoder maps data to a dense representation; a decoder reconstructs the data from this representation. In normal auto encoders, the dense representation can be any layer in a neural network. As we constrain the size of this network, by backpropagation we learn a dense representation of the data.\r\n\r\n##Information layer\r\nIn variational auto encoders, the dense representation is also named information layer. This layer follows Information Theory and we reason accordingly. There is two viewpoints to this layer. They boil down to the same math\r\n  * _The information layer is a mapping to latent space_ The encoder maps the data to this latent space. More specifically, the encoder generates conditional distribution _q(z|x)_ From this conditional distribution, we sample a z for the decoder\r\n  * _The information layer is a noisy connection between encoder and decoder. This article by Dirk Kingma introduces what we call the _reparametrization trick_. For a Gaussian prior on z, the information layer is merely a deterministic function f(z) = mu + sigma*eps, where mu and sigma follow from the encoder. Eps is a draw from a unit Gaussian.\r\n\r\nThe according python code is as follows:\r\n    with tf.name_scope(\"Latent_space\") as scope:\r\n      self.eps = tf.random_normal(tf.shape(self.z_mu),0,1,dtype=tf.float32)\r\n      self.z = self.z_mu + tf.mul(tf.sqrt(tf.exp(z_sig_log_sq)),self.eps)   #Z is the vector in latent space\r\n# Model\r\nThese experiments use an LSTM as recurrent neural network. LSTM can depend on long term information. This is beneficial for VRAE, where the only information arises from the latent space. From the latent space, the model predicts the initial state. Throughout the sequence, the output at every step inputs into the LSTM. This way, the model knows what it just predicted\r\n\r\nTODO: reference Kingma\r\nreference Karol",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}